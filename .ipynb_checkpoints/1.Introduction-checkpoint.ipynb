{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2dd9937-648a-4eb7-9c9d-397876764d99",
   "metadata": {},
   "source": [
    "## Overview of Generative AI Architecture and Security Chanllenges\n",
    "\n",
    "|            **Aspect**            |                         **Description**                        |\n",
    "|:--------------------------------:|:--------------------------------------------------------------:|\n",
    "|          Data Management         | Handling sensitive data in collection, storage and processing. |\n",
    "|          Model Training          | Exposure to poisoned data and tempering risks during training. |\n",
    "|         Inference Process        |       Leakage of sensitive information through outputs.        |\n",
    "|        APIs and Endpoints        |          Vulnerable to unauthorized access and misuse.         |\n",
    "|     Third-Party Dependencies     |     Risks from libraries, frameworks and cloud-based tools.    |\n",
    "|        Adversarial Inputs        |         Inputs crafted to deceive or exploit the model.        |\n",
    "|      Operational Scalability     |    Balancing performance and security when scaling systems.    |\n",
    "| Monitoring and Incident Response |      Detecting threats and addressing breaches effectively     |\n",
    "\n",
    "# AI Security Aspects\n",
    "\n",
    "### Data Management\n",
    "Keeping private or secret information (like your name, photos, health records, or passwords) safe when the AI collects it, saves it, or uses it.\n",
    "\n",
    "### Model Training\n",
    "Protecting the AI while it’s learning, so bad guys can’t sneak in fake or “poisoned” data that makes the AI learn wrong things or have hidden weaknesses.\n",
    "\n",
    "### Inference Process\n",
    "Making sure that when the AI gives answers, it never accidentally leaks private or secret stuff it saw during training.\n",
    "\n",
    "### APIs and Endpoints\n",
    "Securing the “doors” (apps and web links) that people and programs use to talk to the AI, so hackers can’t break in or misuse it.\n",
    "\n",
    "### Third-Party Dependencies\n",
    "Checking that all the outside tools, code libraries, and cloud services the AI uses are safe and don’t have hidden bugs or backdoors.\n",
    "\n",
    "### Adversarial Inputs\n",
    "Defending against specially crafted tricks (weird images or sentences) that look normal to humans but confuse or control the AI.\n",
    "\n",
    "### Operational Scalability\n",
    "Keeping the AI fast and secure even when millions of people are using it at the exact same time.\n",
    "\n",
    "### Monitoring and Incident Response\n",
    "Watching the AI 24/7 for anything strange and having a super-fast plan to fix problems if something bad (hack, leak, attack) happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2bbc7a-e49b-4bb5-9672-a26cada6f60c",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "## Risks Associated With Unsecured GenAI Models and Systems\n",
    "\n",
    "|         **Risk**        |                             **Description**                             |\n",
    "|:-----------------------:|:-----------------------------------------------------------------------:|\n",
    "|      Data Breaches      |         Exposure of sensitive data during training or inference.        |\n",
    "|       Model Theft       |          Unauthorized replication or use of proprietary models.         |\n",
    "|   Adversarial Attacks   |       Manipulated inputs leading to misleading or harmful outputs.      |\n",
    "|     Bias Propagation    |         Reinforcement and amplification of existing data biases.        |\n",
    "|       API Misuse        |            Exploitation of open or poorly secured endpoints.            |\n",
    "| Operational Disruptions |     Service downtime caused by malicious actors or system overloads.    |\n",
    "|   Reputational Damage   |        Loss of trust from stakeholders due to security failures.        |\n",
    "|   Regulatory Penalties  | Non-Compliance with legal data protection standards resulting in fines. |\n",
    "\n",
    "\n",
    "## AI Security Risks\n",
    "\n",
    "### Data Breaches  \n",
    "Private or secret information (like names, passwords, health records) gets stolen or accidentally leaked while the AI is learning or answering questions.\n",
    "\n",
    "### Model Theft  \n",
    "Someone steals the entire AI brain (the trained model) so they can copy it, sell it, or use it without permission.\n",
    "\n",
    "### Adversarial Attacks  \n",
    "Hackers send special “trick” inputs (images, text, or audio) that look normal to us but force the AI to give wrong, dangerous, or silly answers.\n",
    "\n",
    "### Bias Propagation  \n",
    "If the training data already has unfair stereotypes (about race, gender, etc.), the AI learns them and makes the unfairness even worse when it answers.\n",
    "\n",
    "### API Misuse  \n",
    "Bad people abuse the AI’s “doors” (APIs) – for example, spamming it, trying to break it, or using it to create harmful content.\n",
    "\n",
    "### Operational Disruptions  \n",
    "Attackers crash the AI service or overload it on purpose so nobody can use it (like turning off the lights in a store).\n",
    "\n",
    "### Reputational Damage  \n",
    "When something bad happens (leak, wrong answer, hack), people stop trusting the company that made the AI.\n",
    "\n",
    "### Regulatory Penalties  \n",
    "The AI breaks privacy laws (like GDPR or CCPA), so the government fines the company a lot of money.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d5a31f-9b23-4b7e-944c-4d76716916ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
