{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e6dd799-a5bb-47fb-b40c-f8ed7653489a",
   "metadata": {},
   "source": [
    "# Foundations of AI Security\n",
    "\n",
    "## Why AI Security Matters\n",
    "Generative AI (like ChatGPT, Claude, Grok, etc.) uses huge amounts of data and is open to everyone on the internet → new risks appear that normal software doesn’t have.  \n",
    "This section teaches the basic building blocks to keep AI safe and trustworthy.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Data Privacy & Confidentiality\n",
    "**Simple meaning:** Make sure private information (names, health records, credit cards, chats) never leaks when the AI is learning or answering questions.\n",
    "\n",
    "#### Key Ways to Protect Data\n",
    "| Method                        | What it does (easy words)                                      | Real example from the text (\"Secure AI Solutions\")                              |\n",
    "|-------------------------------|----------------------------------------------------------------|---------------------------------------------------------------------------|\n",
    "| Encryption at rest            | Locks the data when it's saved on disk (like a safe)           | Customer logs from banks & hospitals are encrypted in their cloud storage |\n",
    "| Role-Based Access Control (RBAC) | Only the right people can see or touch the data               | Only senior engineers can open the training data; interns cannot         |\n",
    "| Anonymization / Differential Privacy | Hides real identities even if someone looks at the answers    | Retail chatbot removes names & phone numbers before replying              |\n",
    "| Regular Audits & Compliance   | Checks every few months that rules (GDPR, etc.) are followed   | They review everything to avoid big fines from Europe                     |\n",
    "\n",
    "Real-life danger if ignored → AI might accidentally repeat someone’s address or medical history in a normal conversation!\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Model Integrity & Robustness\n",
    "**Simple meaning:** Make sure the AI cannot be tricked or forced to say wrong, harmful, or stupid things.\n",
    "\n",
    "#### Biggest Threat: Adversarial Attacks\n",
    "Hackers send special “trick” images or messages that look normal to humans but completely confuse the AI.\n",
    "\n",
    "#### How to Defend (Easy Methods)\n",
    "| Method                     | What it does                                                  | Example from Secure AI                                      |\n",
    "|----------------------------|---------------------------------------------------------------|-------------------------------------------------------------|\n",
    "| Adversarial Training       | Show the AI thousands of trick examples while teaching it    | They feed fake bad inputs so the chatbot learns to say “No” |\n",
    "| Robustness Testing         | Test with weird & extreme inputs before launch                | Test if summarizer messes up financial reports              |\n",
    "| Real-Time Monitoring       | Watch live traffic for suspicious patterns                    | Flags 1000 identical questions trying to steal information  |\n",
    "| Encrypt Model Weights      | Lock the AI brain so nobody can steal or change it            | Healthcare chatbot model is encrypted – can’t be copied     |\n",
    "\n",
    "Real-life danger if ignored → A customer service bot could be tricked into giving racist answers or leaking internal prices.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Access Control & Authentication\n",
    "**Simple meaning:** Only let the right people (and programs) touch the AI system.  \n",
    "Golden rule = Principle of Least Privilege → give the smallest power needed.\n",
    "\n",
    "#### Most Common Tools\n",
    "| Tool / Technique                 | Easy Explanation                                                      | Example Use                                                  |\n",
    "|----------------------------------|-----------------------------------------------------------------------|--------------------------------------------------------------|\n",
    "| Role-Based Access Control (RBAC) | Different roles → different permissions (admin, developer, viewer)   | Marketing team can only chat; engineers can update the model |\n",
    "| Multi-Factor Authentication (MFA) | Password + phone code or authenticator app                           | Everyone must use Google Authenticator to log in             |\n",
    "| API Keys + Scoped Tokens         | Special secret keys that only allow certain actions                   | External app can only send 1000 messages/day                 |\n",
    "\n",
    "Hands-on activity mentioned:  \n",
    "You will set up RBAC policies for a CI/CD system (the system that deploys new AI versions) so only trusted people can push updates.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary – What You Will Know After This Section\n",
    "- How to stop private data from leaking (training + chatting stage)  \n",
    "- How to stop hackers from tricking your AI with weird inputs  \n",
    "- How to lock doors so only authorized people touch the AI  \n",
    "- Real examples and simple techniques you can use tomorrow\n",
    "\n",
    "You’ll even get to practice by setting up real access rules!\n",
    "\n",
    "Ready to become the person who keeps the AI safe?  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
